{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "ed3cab8d-3fba-4d63-ba7b-048b462dac38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div  style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/derar-alhussein/Databricks-Certified-Data-Engineer-Professional/main/Includes/images/orders.png\" width=\"60%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd67233-d6eb-45ff-9c1c-b6d6d6ad008d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data catalog: workspace\nSchema: bookstore_eng_pro\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Copy-Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8122cff1-46e2-4e76-bb59-52cb6caa7bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(spark.read\n",
    "      .table(\"bronze\")\n",
    "      .filter(\"topic = 'orders'\")\n",
    "      .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cee3f30c-c0d8-4631-bf2f-f98853837cdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "json_schema = \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\n",
    "\n",
    "batch_total = (spark.read\n",
    "                      .table(\"bronze\")\n",
    "                      .filter(\"topic = 'orders'\")\n",
    "                      .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                      .select(\"v.*\")\n",
    "                      .dropDuplicates([\"order_id\", \"order_timestamp\"])\n",
    "                      .count()\n",
    "                )\n",
    "\n",
    "print(batch_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eefce950-e717-47a8-a7c9-1331bda8ee23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deduped_df = (spark.readStream\n",
    "                   .table(\"bronze\")\n",
    "                   .filter(\"topic = 'orders'\")\n",
    "                   .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                   .select(\"v.*\")\n",
    "                   .withWatermark(\"order_timestamp\", \"30 seconds\")\n",
    "                   .dropDuplicates([\"order_id\", \"order_timestamp\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0916145-4ad5-4eb4-a4ae-8a87c6fc9343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_data(microBatchDF, batch):\n",
    "    microBatchDF.createOrReplaceTempView(\"orders_microbatch\")\n",
    "    \n",
    "    sql_query = \"\"\"\n",
    "      MERGE INTO orders_silver a\n",
    "      USING orders_microbatch b\n",
    "      ON a.order_id=b.order_id AND a.order_timestamp=b.order_timestamp\n",
    "      WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\"\n",
    "    \n",
    "    microBatchDF.sparkSession.sql(sql_query)\n",
    "    #microBatchDF._jdf.sparkSession().sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98b4b5ec-cf23-44c7-9eaf-928a5d007b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS orders_silver\n",
    "(order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd1c821-ad30-4d2b-bc5f-15bd80629cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (deduped_df.writeStream\n",
    "                   .foreachBatch(upsert_data)\n",
    "                   .option(\"checkpointLocation\", f\"{bookstore.checkpoint_path}/orders_silver\")\n",
    "                   .trigger(availableNow=True)\n",
    "                   .start())\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f1ac9fb-74f1-44b2-a2ed-eda566c3de40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch total: 800\nstreaming total: 800\n"
     ]
    }
   ],
   "source": [
    "streaming_total = spark.read.table(\"orders_silver\").count()\n",
    "\n",
    "print(f\"batch total: {batch_total}\")\n",
    "print(f\"streaming total: {streaming_total}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8171290041955496,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.4 - Streaming Deduplication",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}